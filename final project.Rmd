---
title: "Capstone Project"
output: html_notebook
---

```{r}
install.packages("ggplot2")
install.packages("corrplot")
install.packages("gridExtra")
install.packages("party")
installed.packages("class")
installed.packages("gmodels")
library(gridExtra)
library(corrplot)
library(class)
library(gmodels)
library(party)
library(ggplot2)
library(lubridate)
library(tidyverse)
library(caTools)
library(GGally)
```


```{r}
project=read.csv("C:/Users/dell/Downloads/archive/kc_house_data.csv", 
                stringsAsFactors= FALSE, header = TRUE, sep = ",")
```

```{r}
head(project,10)
```


```{r}
str(project)
```

```{r}
summary(project)
```
# Data Cleaning
```{r}
project$date = as.Date(project$date)
str(project)

```

```{r}
dim(project)
```

```{r}
NA_values=data.frame(no_of_na_values=colSums(is.na(project)))
head(NA_values,21)
```
```{r}
sum(NA_values)
```

```{r}
which(is.na(project))
```

```{r}
gradeMean=mean(project$grade, na.rm = TRUE)  #indexing
project[is.na(project$grade),"grade"]=gradeMean
project$grade = as.integer(project$grade)
View(project$grade)
```

```{r}
sum(is.na(project))
```

#removing the column which do not provide any information about housing
```{r}
project$id = NULL
View(project)
```

#converting the price from Dollar to units of 1000 Dollar to improve readability.
```{r}
project$price = project$price / 1000
```

#density of the price to get a first impression on its distribution
```{r}
ggplot(project, aes(x = price)) + geom_density()
```

```{r}
range(project$price)
#min(range(project$price))
#max(range(project$price))
```

# Dividing data into test and train set
```{r}
set.seed(1)
sample <- sample(c(TRUE, FALSE), nrow(project), replace=TRUE, prob=c(0.7,0.3))
#  set seed to ensure you always have same random numbers generated
#sample = sample.split(project,SplitRatio = 0.8) # splits the data in the ratio mentioned in SplitRatio. After splitting marks these rows as logical TRUE and the the remaining are marked as logical FALSE
#train_data =subset(project,sample ==TRUE) # creates a training dataset named train1 with rows which are marked as TRUE
#test_data=subset(project, sample==FALSE)
train_data<- project[sample, ]
test_data<- project[!sample, ]
```

#Determining the association between variables.
```{r}
cor_data=data.frame(train_data[,3:19])
correlation=cor(cor_data)
corrplot(correlation,method="color",outline="black", insig = "p-value")
```
#According to our corrplot price is positively correlated with bedroom, bathroom, Sqft_living, view , grade, sqft_above, sqft_basement, lat, sqft_living 15.
#Next we will draw some scatter plots to determine the relationship between these variables.
```{r}

p1=ggplot(data = train_data, aes(x = bedrooms, y = price)) +  geom_smooth(method = "lm", se = FALSE)+labs(title="Scatter plot of Bedrooms and Price", x="bedrooms",y="Price")
p2=ggplot(data = train_data, aes(x = bathrooms, y = price))  +  geom_smooth(method = "lm", se = FALSE)+labs(title="Scatter plot of Bathrooms and Price", x="bathrooms",y="Price")
p3=ggplot(data = train_data, aes(x = sqft_living, y = price)) +  geom_smooth(method = "lm", se = FALSE)+labs(title="Scatter plot of Sqft_living and Price", x="Sqft_living",y="Price")
p4=ggplot(data = train_data, aes(x = sqft_above, y = price)) +  geom_smooth(method = "lm", se = FALSE)+labs(title="Scatter plot of Sqft_above and Price", x="Sqft_above",y="Price")
p5=ggplot(data = train_data, aes(x = sqft_basement, y = price)) +  geom_smooth(method = "lm", se = FALSE)+labs(title="Scatter plot of Sqft_basement and Price", x="Sqft_basement",y="Price")
p6=ggplot(data = train_data, aes(x = lat, y = price)) +  geom_smooth(method = "lm", se = FALSE)+labs(title="Scatter plot of Latitude and Price", x="Latitude",y="Price")
p7=ggplot(data = train_data, aes(x = sqft_living15, y = price)) +  geom_smooth(method = "lm", se = FALSE)+labs(title="Scatter plot of Sqft_living15 and Price", x="Sqft_living15",y="Price")
grid.arrange(p1,p2,p3,p4,p5,p6,p7,nrow=4)
```

#from these scatter plots, we conclude that the relationship between price and bedroom, bathroom, Sqft_living,sqft_above, sqft_basement, lat, sqft_living 15 is linear.
#For the two categorical variables(view and grade) we draw boxplots to understand the relationship.
```{r}
#par(mfrow=c(1, 2))
boxplot(price~view,data=train_data,main="Price vs View boxplot", xlab="view",ylab="price",col="orange",border="brown")
boxplot(price~grade,data=train_data,main="Grade vs Price boxplots", xlab="grade",ylab="price",col="orange",border="brown")
```

#now we check for outliers in the dependent variable(price) using a boxplot.
```{r}
ggplot(data=train_data)+geom_boxplot(aes(x=bedrooms,y=price))
```
#we see that we have a significantly large number of outliers.
#Treating or altering the outlier/extreme values in genuine observations is not a standard operating procedure. However, it is essential to understand their impact on our predictive models.
#To better understand the implications of outliers better, I am going to compare the fit of a simple linear regression model on the dataset with and without outliers.
#For this we first extract outliers from the data and then obtain the data without the outliers.




#adding two new columns for our better understanding.
###price might have a fair chance of depending on the age of the house and also the number of times it has been renovated.So we try to extact the age and the number of times a particular house has been renovated from our train data.
```{r}
train_data$age=train_data$date-train_data$yr_built #age of the house
train_data$reno=ifelse(train_data$yr_renovated==0,0,1) # number of times renovated
train_data$reno=as.factor(train_data$reno)
test_data$age=test_data$date-test_data$yr_built #age of the house
test_data$reno=ifelse(test_data$yr_renovated==0,0,1) # number of times renovated
test_data$reno=as.factor(test_data$reno)
```

```{r}
outliers=boxplot(train_data$price,plot=FALSE)$out
outliers_data=train_data[which(train_data$price %in% outliers),]
train_data1= train_data[-which(train_data$price %in% outliers),]
summary(train_data1)
```

#we obtain 872 observations as outliers.
#Now we plot the data with and without outliers.

```{r}
par(mfrow=c(1, 2))
plot(train_data$bedrooms, train_data$price, main="With Outliers", xlab="bedrooms", ylab="price", pch="*", col="red", cex=2)
abline(lm(price ~ bedrooms, data=train_data), col="blue", lwd=3, lty=2)
# Plot of original data without outliers. Note the change of slope.
plot(train_data1$bedrooms, train_data1$price, main="Outliers removed", xlab="bedrooms", ylab="price", pch="*", col="red", cex=2)
abline(lm(price ~bedrooms, data=train_data1), col="blue", lwd=3, lty=2)
```
#Notice the change in slope of the best fit line after removing the outliers. It is evident that if we remove the outliers to train the model, our predictions would be exagerated (high error) for larger values of price because of the larger slope.

#MODELING
```{r}
model=lm(data=train_data,price~bedrooms+bathrooms+sqft_living+view+grade+sqft_above+sqft_basement+sqft_living15)
summary(model)

```
```{r}
#use model to predict probability of default
predicted <- predict(model, test_data, type="response")
predicted_class<-round(predicted)
test_data$class=predicted_class
conf_matrix<-table(actual=test_data$class,predicted=predicted_class)
conf_matrix
```

####We can see the relationship between these variables appear to be moderately strong as shown by R-Suared value and the probability.also coclude from the p-value that sqft_living15 is not a significant variable for the prediction of price. Hence we drop it.
```{r}
model2=lm(data=train_data1,price~bedrooms+bathrooms+sqft_living+view+grade+sqft_lot+age+floors+waterfront)
summary(model2)
```
```{r}
predicted <- predict(model2, test_data, type="response")
predicted_class<-round(predicted)
test_data$class=predicted_class
conf_matrix<-table(actual=test_data$class,predicted=predicted_class)
conf_matrix
```




#As concluded from the adjusted R-squared value of 0.4855, the relationship beween these variables appear to be quite strong.



```{r}
unique(project$condition)
```


```{r}
#Logistic Regression
model3<-glm(data=train_data1,price~bedrooms+bathrooms+sqft_living+view+grade+sqft_lot+age+floors+waterfront)
price<-coef(model3)[1]+coef(model3)[2]*project$bedrooms+coef(model3)[3]*project$bathrooms+coef(model3)[4]*project$sqft_living+coef(model3)[5]*project$view+coef(model3)[6]*project$grade+coef(model3)[7]*project$sqft_lot+coef(model3)[8]*project$age+coef(model3)[9]*project$floors+coef(model3)[10]*project$waterfront
summary(model3)
range(train_data$condition)
min(range(train_data$condition))
max(range(train_data$condition))
```
```{r}
x<- seq(min(range(project$condition)),max(range(project$condition)),0.1)
x
y<-predict(model,list(condition=x),type="response")
y
plot(project$condition,project$price)

```
```{r}
plot(x,y)
lines(x,y)
```

```{r}
#predict(model, train_data,type = "response")
predicted<-predict(model3,train_data,type="response")
#predicted
predicted_class<-round(predicted)
train_data$class=predicted_class
conf_matrix<-table(actual=train_data$class,predicted=predicted_class)
conf_matrix
accuracy<-sum(diag(conf_matrix))/sum(conf_matrix)
accuracy
```


We can see the relationship between these variables appear to be moderately strong as shown by R-Squared value and the probability.also coclude from the p-value that sqft_living15 is not a significant variable for the prediction of price. Hence we drop it. ####We also try fitting the model including a few other variables which we left out in the EDA and stop at a model which gives us the maximum R-squared value.


#making decision trees
```{r}
#library(party)
project_ctree1 <- ctree(condition~price,data=train_data)
plot(project_ctree1, type="simple")

#using decision trees for training and test set
#train_index1=sample(1:nrow(train_data),0.7*nrow(project))
train_index1=sample(c(TRUE, FALSE), nrow(project), replace=TRUE, prob=c(0.7,0.3))
train_set1=train_data[train_index1,]
test_set1=train_data[-train_index1,]

#run model on training set
project_ctree_model1 <- ctree(condition~price,data=train_data)
project_ctree_model1
```
```{r}
#prediction on test set
project_ctree_prediction1 <- predict(project_ctree_model1,test_data)
head(project_ctree_prediction1)

#confusion matrix
table(project_ctree_prediction1,test_data$condition)
```


```{r}

indep_train_data=train_data[-5]# making a independent set by removing target variable
indep_test_data=test_data[-5]
indep_test_data
target_train_data=train_data$sqft_living
target_test_data=test_data$sqft_living



# applying KNN
pred_target_test_set1=knn(indep_train_set1,indep_test_set1,target_train_set1,k=3) 
table(pred_target_test_set1,target_test_set1)

```
#applying kmean

```{r}
head(train_data)
project=project[-1]
kmean_housing=kmeans(project,5)
kmean_housing
table(project$condition,kmean_housing$cluster)

```
```{r}
library(fpc)
plotcluster(project, kmean_housing$cluster)
```
```{r}
decompose(project)
```


